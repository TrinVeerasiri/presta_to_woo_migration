{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer migration from Prestashop to Woocommerce part 1 : Gathering raw customer information from Prestashop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the product migration of the book store from Prestashop to Woocommerce format. Unlike the product, users (customers in Woocommerce are called users) don't have an import function. We must import the data via sql. Woocommerce users database structure consists of 2 sql table \"wp_users\" and \"wp_usermeta\". \n",
    "\n",
    "1) \"wp_users\" is about the name, username and password of the users\n",
    "2) \"wp_usermeta\" is about the detail of users such as billing address, group of users etc.\n",
    "\n",
    "Our objective here is to collect the data from Prestashop, manipulate them in the form of Woocommerce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from Prestashop backend and sql. Note that Prestashop backend can generate some table for you by a default function. We copy and paste into excel (the file will have .xlsx extension). For our comfortable, we use this table as a starter then merge with the other missing information from sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read data from excel\n",
    "#data from prestashop backend\n",
    "customer_from_backend = pd.read_excel('sql_prestashop/customer_detail.xlsx')\n",
    "address_from_backend = pd.read_excel('sql_prestashop/customer_address.xlsx')\n",
    "#data from mysql database\n",
    "ps_address = pd.read_csv('sql_prestashop/ps_address.csv', index_col=False)\n",
    "ps_customer_group = pd.read_csv('sql_prestashop/ps_customer_group.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only interest column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select only use column\n",
    "ps_address = ps_address[['id_address', 'id_customer', 'other', 'phone', 'phone_mobile', 'vat_number', 'date_add', 'date_upd', 'active', 'deleted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "address_from_backend = address_from_backend.drop(['firstname','lastname'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort id_customer ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#customer_from_backend is not ascending\n",
    "customer_from_backend = customer_from_backend.sort_values('id_customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one customer may has many id_address. This depend on how many times they change the address on frontend. We create \"link_id_and_address\" dataframe to link \"id_customer\" and \"id_address\" in order to find the unique id_customer with max id_address first (max id is the lastest address available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_id_and_address = pd.DataFrame()\n",
    "link_id_and_address['id_customer'] = ps_address['id_customer']\n",
    "link_id_and_address['id_address'] = ps_address['id_address']\n",
    "#sort \"id_customer\".\n",
    "link_id_and_address = link_id_and_address.sort_values('id_customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group \"link_id_and_address\" by \"id_customer\", so we have the unique \"id_customer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assume max id_address to be a lastest address update, so we use groupby to \n",
    "#find lastest id_address\n",
    "link_id_and_address = link_id_and_address.groupby('id_customer', as_index=False).max()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we merge all dataframe into \"information\" dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge\n",
    "information = pd.merge(link_id_and_address, customer_from_backend, how='left', on='id_customer')\n",
    "information = pd.merge(information, address_from_backend, how='left', on='id_address')\n",
    "information = pd.merge(information, ps_address, how='left', on='id_address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change name of column in order to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge column with the same name will generate _x or _y follow the column name\n",
    "#So rename it first for further use.\n",
    "information = information.rename(columns = {'id_customer_x':'id_customer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prestashop stores full name country. We must change to iso country format to use in Woocommerce. We view how many country first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Changing country in to woocommerce format\n",
    "count_country = information['country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load iso country reference in to a dic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load iso country reference data from Wikipedia\n",
    "dic = {}\n",
    "with open(\"sql_prestashop/wikipedia-iso-country-codes.csv\") as f:\n",
    "    file= csv.DictReader(f, delimiter=',')\n",
    "    for line in file:\n",
    "        dic[line['English short name lower case']] = line['Alpha-2 code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If country column has nan value, it will cause an error. We view the country column first. Are there any nan value? Then temporary fill it with '-'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#country column has nan value. change it to other country, avoid the error.\n",
    "#count null country\n",
    "information['country'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temporary fill it with '-'\n",
    "information['country'] = information['country'].fillna('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing some country names don't match with name in country dic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add dic for '-'\n",
    "dic['-'] = '-'\n",
    "#Changing some dics that don't match the iso code\n",
    "dic['Taiwan'] = dic['Taiwan, Province of China']\n",
    "dic['South Korea'] = dic[\"Korea, Republic of\"]\n",
    "dic['HongKong'] = dic['Hong Kong']\n",
    "dic['Vietnam'] = dic['Viet Nam']\n",
    "dic['Korea, Dem. Republic of'] = dic[\"Korea, Democratic People's Republic of\"]\n",
    "dic['Macau'] = dic[\"Macao\"]\n",
    "dic['Brunei'] = dic[\"Brunei Darussalam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing to iso country code. Fill '-' back with nan value. Then check for nan value again. The number of nan value must equal to the previous check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change to iso code\n",
    "information['country'] = [dic[x] for x in information['country']]\n",
    "#Replace '-' back to ba nan value\n",
    "information['country'] = information['country'].replace('-',np.NaN)\n",
    "#Check number of nan value again\n",
    "information['country'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In new Woocommerce store, we have 3 groups of user. That is 18+, vendor and customer. The old Prestashop customers we have 7 group. The groups we want to keep are group 6 (vendors), 4 (18+) and the rest is 3 (customers).\n",
    "\n",
    "For our comfortable running the for loop, we change the max group number (7) to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean data for ps_customer group\n",
    "#Group 6 is first priority then 4 \n",
    "#First change group 7 to 0 in order to make group 6 is the max value\n",
    "for x in range(ps_customer_group.shape[0]):\n",
    "    if ps_customer_group['id_group'].iloc[x] == 7:\n",
    "        ps_customer_group['id_group'].iloc[x] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the maximum group number is 6. We group customer by \"id_customer\" by max group number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find max value of group id       \n",
    "group = ps_customer_group.groupby('id_customer', as_index=False).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still have group number lower than 3. We set them to be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Still have group 2. Change it to group 3\n",
    "for x in range(group.shape[0]):\n",
    "    if group['id_group'].iloc[x] == 2:\n",
    "        group['id_group'].iloc[x] = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have customer in 3 groups\n",
    "\n",
    "1) Group 6 Vendors\n",
    "\n",
    "2) Group 4 18+\n",
    "\n",
    "3) Group 3 Customers\n",
    "\n",
    "Then we merge \"group\" to infromation dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge again\n",
    "information = pd.merge(information, group,\\\n",
    "                       how='left', on='id_customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for nan value in \"id_group\" and fill it with '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check for nan value\n",
    "information['id_group'].isnull().sum()\n",
    "#Fill nan with 3\n",
    "information['id_group'] = information['id_group'].fillna(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_import_to_woo/raw_information.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7202c7eea910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#save to .csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minformation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'customer_import_to_woo/raw_information.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1576\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customer_import_to_woo/raw_information.csv'"
     ]
    }
   ],
   "source": [
    "#save to .csv\n",
    "information.to_csv('customer_import_to_woo/raw_information.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
